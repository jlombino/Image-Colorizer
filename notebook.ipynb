{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vram = 14 # GB\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.set_logical_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.LogicalDeviceConfiguration(memory_limit=vram*1024)])\n",
    "print(len(gpus), \"Physical GPUs Configured\")\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_images(count=1000,image_dir='images/',image_size=256):\n",
    "    image_array = []\n",
    "    for image_file in os.listdir(image_dir):\n",
    "        if len(image_array) >= count:\n",
    "            break\n",
    "        else:\n",
    "            image = cv2.imread(image_dir+image_file)\n",
    "            image = cv2.resize(image,(image_size,image_size))\n",
    "            image_array.append(image)\n",
    "    return np.array(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_set,test_fraction=0.05):\n",
    "    l_array = []\n",
    "    lab_array = []\n",
    "\n",
    "    for image in image_set:\n",
    "        lab_image = cv2.cvtColor(image,cv2.COLOR_RGB2LAB)\n",
    "        l_channel, a_channel, b_channel = cv2.split(lab_image)\n",
    "\n",
    "        l_channel = l_channel/255\n",
    "        a_channel = (a_channel-128)/255\n",
    "        b_channel = (b_channel-128)/255\n",
    "        recombined = np.dstack((a_channel,b_channel))\n",
    "\n",
    "        l_array.append(np.expand_dims(l_channel,axis=2))\n",
    "        lab_array.append(recombined)\n",
    "\n",
    "    return train_test_split(np.array(l_array), np.array(lab_array),test_size=test_fraction,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_and_y_to_rgb(gray,colors):\n",
    "    a_channel = colors [:,:,0]\n",
    "    b_channel = colors [:,:,1]\n",
    "    l_channel = gray * 255\n",
    "    a_channel = (a_channel*255)+128\n",
    "    b_channel = (b_channel*255)+128\n",
    "    recombined = np.dstack((l_channel,a_channel,b_channel))\n",
    "    converted = cv2.cvtColor(recombined.astype(np.uint8),cv2.COLOR_LAB2RGB)\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(image_size=256):\n",
    "\n",
    "    inputs = layers.Input(shape=(image_size,image_size,1))\n",
    "\n",
    "    down1 = layers.Conv2D(32,kernel_size=(5,5),padding='same')(inputs)\n",
    "    down1 = layers.BatchNormalization()(down1)\n",
    "    down1 = layers.LeakyReLU()(down1)\n",
    "    down1 = layers.Conv2D(32,kernel_size=(5,5),padding='same')(down1)\n",
    "    down1 = layers.BatchNormalization()(down1)\n",
    "    down1 = layers.LeakyReLU()(down1)\n",
    "\n",
    "    down2 = layers.Conv2D(64,kernel_size=(5,5),padding='same')(down1) \n",
    "    down2 = layers.BatchNormalization()(down2)\n",
    "    down2 = layers.LeakyReLU()(down2)\n",
    "    down2 = layers.Conv2D(64,kernel_size=(5,5),padding='same')(down2) \n",
    "    down2 = layers.BatchNormalization()(down2)\n",
    "    down2 = layers.LeakyReLU()(down2)\n",
    "\n",
    "    down3 = layers.Conv2D(128,kernel_size=(5,5),padding='same')(down2) \n",
    "    down3 = layers.BatchNormalization()(down3)\n",
    "    down3 = layers.LeakyReLU()(down3)\n",
    "    down3 = layers.Conv2D(128,kernel_size=(5,5),padding='same')(down3) \n",
    "    down3 = layers.BatchNormalization()(down3)\n",
    "    down3 = layers.LeakyReLU()(down3)\n",
    "\n",
    "    bottleneck = layers.Conv2D(256,kernel_size=(5,5),padding='same')(down3)\n",
    "    bottleneck = layers.BatchNormalization()(bottleneck)\n",
    "    bottleneck = layers.LeakyReLU()(bottleneck)\n",
    "\n",
    "    up1 = layers.Concatenate()([bottleneck,down3])\n",
    "    up1 = layers.Conv2DTranspose(128,kernel_size=(5,5),padding='same')(up1)\n",
    "    up1 = layers.BatchNormalization()(up1)\n",
    "    up1 = layers.LeakyReLU()(up1)\n",
    "    up1 = layers.Conv2DTranspose(128,kernel_size=(5,5),padding='same')(up1)\n",
    "    up1 = layers.BatchNormalization()(up1)\n",
    "    up1 = layers.LeakyReLU()(up1)\n",
    "\n",
    "    up2 = layers.Concatenate()([up1,down2])\n",
    "    up2 = layers.Conv2DTranspose(64,kernel_size=(5,5),padding='same')(up2)\n",
    "    up2 = layers.BatchNormalization()(up2)\n",
    "    up2 = layers.LeakyReLU()(up2)\n",
    "    up2 = layers.Conv2DTranspose(64,kernel_size=(5,5),padding='same')(up2)\n",
    "    up2 = layers.BatchNormalization()(up2)\n",
    "    up2 = layers.LeakyReLU()(up2)\n",
    "\n",
    "    up3 = layers.Concatenate()([up2,down1])\n",
    "    up3 = layers.Conv2DTranspose(32,kernel_size=(5,5),padding='same')(up3)\n",
    "    up3 = layers.BatchNormalization()(up3)\n",
    "    up3 = layers.LeakyReLU()(up3)\n",
    "    up3 = layers.Conv2DTranspose(32,kernel_size=(5,5),padding='same')(up3)\n",
    "    up3 = layers.BatchNormalization()(up3)\n",
    "    up3 = layers.LeakyReLU()(up3)\n",
    "\n",
    "    outputs = layers.Conv2DTranspose(2,kernel_size=(5,5),padding='same',activation='sigmoid')(up3)\n",
    "\n",
    "    model = keras.models.Model(inputs,outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator(image_size=256):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(32,kernel_size=(5,5),padding='same',input_shape=(image_size,image_size,2)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Conv2D(32,kernel_size=(5,5),padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.MaxPool2D())\n",
    "\n",
    "    model.add(layers.Conv2D(64,kernel_size=(5,5),padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Conv2D(64,kernel_size=(5,5),padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.MaxPool2D())\n",
    "    \n",
    "    model.add(layers.Conv2D(128,kernel_size=(5,5),padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Conv2D(128,kernel_size=(5,5),padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.MaxPool2D())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(64))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(32))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    cross_entropy = losses.BinaryCrossentropy()\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output),real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output),fake_output)\n",
    "    total_loss = real_loss + fake_loss   \n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output,gray_batch,color_batch):\n",
    "    cross_entropy = losses.BinaryCrossentropy()\n",
    "    mae = losses.MeanAbsoluteError()\n",
    "    disc_loss = cross_entropy(tf.ones_like(fake_output),fake_output)\n",
    "    gen_loss = tf.cast(mae(gray_batch,color_batch),'float32')\n",
    "    total_loss = disc_loss + gen_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gen_step(gray_batch,color_batch):\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_batch = generator(gray_batch,training=True)\n",
    "        generated_disc_output = discriminator(generated_batch,training=False)\n",
    "        gen_loss = generator_loss(generated_disc_output,gray_batch,color_batch)\n",
    "      \n",
    "    generator_gradients = gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,generator.trainable_variables))\n",
    "\n",
    "    gen_loss_tracker(gen_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def disc_step(gray_batch,color_batch):\n",
    "    \n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        generated_batch = generator(gray_batch,training=False)\n",
    "        generated_disc_output = discriminator(generated_batch,training=True)\n",
    "        real_disc_output = discriminator(color_batch,training=True)\n",
    "        disc_loss = discriminator_loss(real_disc_output,generated_disc_output)\n",
    "    \n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,discriminator.trainable_variables))\n",
    "\n",
    "    disc_loss_tracker(disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs,dataset,save=False):\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        for gray_batch,color_batch in dataset:\n",
    "            disc_step(gray_batch,color_batch)\n",
    "\n",
    "        with disc_summary_writer.as_default():\n",
    "            tf.summary.scalar('disc_loss',disc_loss_tracker.result(),step=epoch)\n",
    "\n",
    "        for gray_batch,color_batch in dataset:\n",
    "            gen_step(gray_batch,color_batch)\n",
    "\n",
    "        with gen_summary_writer.as_default():\n",
    "            tf.summary.scalar('gen_loss',gen_loss_tracker.result(),step=epoch)\n",
    "\n",
    "        if save:\n",
    "            if not (epoch+1) % 10:\n",
    "                generator.save(f'models/generator{epoch+1}.hdf5')\n",
    "                discriminator.save(f'models/discriminator{epoch+1}.hdf5')\n",
    "\n",
    "        disc_loss_tracker.reset_states()\n",
    "        gen_loss_tracker.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_images(X,y,count=5):\n",
    "\n",
    "    y_pred = generator(X[:count]).numpy()\n",
    "\n",
    "    fig,ax = plt.subplots(count,3,figsize=(12,4*count))\n",
    "    for idx,row in enumerate(ax):\n",
    "        \n",
    "        row[0].imshow(X[idx],cmap='gray')\n",
    "        row[1].imshow(X_and_y_to_rgb(X[idx],y_pred[idx]))\n",
    "        row[2].imshow(X_and_y_to_rgb(X[idx],y[idx]))\n",
    "\n",
    "        for axis in row:\n",
    "            axis.set_xticks([])\n",
    "            axis.set_yticks([])\n",
    "    fig.set_facecolor('#FFFFFF')\n",
    "    ax[0][0].set_title('Input',fontsize=32);\n",
    "    ax[0][1].set_title('Colorized',fontsize=32);\n",
    "    ax[0][2].set_title('True',fontsize=32);\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator(image_size=64)\n",
    "generator_optimizer = optimizers.Adam(0.0005)\n",
    "gen_loss_tracker = metrics.Mean('gen_loss',dtype=tf.float32)\n",
    "gen_dir = 'logs/generator'\n",
    "gen_summary_writer = tf.summary.create_file_writer(gen_dir)\n",
    "\n",
    "discriminator = make_discriminator(image_size=64)\n",
    "discriminator_optimizer = optimizers.Adam(0.0005)\n",
    "disc_loss_tracker = metrics.Mean('disc_loss',dtype=tf.float32)\n",
    "disc_dir = 'logs/discriminator'\n",
    "disc_summary_writer = tf.summary.create_file_writer(disc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(generator,dpi=60,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(discriminator,dpi=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_n_images(20000,image_size=64)\n",
    "X_train, X_test, y_train, y_test = preprocess_images(images,test_fraction=0.01)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(100,dataset=dataset,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_test_images(X_test,y_test,20);"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "820d50b626c26dd23e815fdc6f0defc021162c216bd79fa699f42d79cf8fcc33"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
