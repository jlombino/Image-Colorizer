{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Colorizer Project *Very work in progress*</h1>\n",
    "<h3>Jason Lombino - Capstone Project Submission</h3><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import colorizerutils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import losses\n",
    "from keras.models import load_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=resources/bentocolor_logo.png>\n",
    "\n",
    "<p>\n",
    "BentoColor is widely known thoughout the broadcasting industry for their colorization of classic TV shows and movies.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Given the recent streaming boom, every broadcasting company is racing to add their entire library of shows and movies to their respective platform. In order to expand libraries and make their paltforms more appealing, BentoColor is recommending that companies add popular pre-color shows and movies to their services.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "While BentoColor has dozens of talented artists ready to work on these projects, the recent explosion of AI art has led BentoColor to question whether AI could be an effective tool in colorizing old movies and TV shows. A successful implementation of AI colorization tools can greatly assist artists in reviving old black and white series and movies. With enough investment into these technologies, it is possible that the AI colorization tools can perform a great deal of the artist's responsibilities allowing to tackle more works in the same period of time.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "BentoColor aims to become a leader in the field of colorization by leveraging their artists' increased productivity to acquire more lucrative colorization contracts from major players in the broadcasting business.\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Getting Colorization Working</h1>\n",
    "\n",
    "<h3>The Current Situation</h3>\n",
    "<p>\n",
    "Colorization is the process of converting a black and white image or video into a partially or fully colored one. There are many techniques currently in use to accomplish this process for videos. Manual colorization where artists color each frame or image individually has mostly fallen out of favor due to advancements in colorization algorithms. Typically now, artists or technicians are able to assign colors for regions of the frame and an algorithm will track that region between frames and continue applying that same color or variations of it depending on brightness. This is still very time consuming for technicians because they need to assign the colors to each of the initial frames in each scene and then to each object or section that enters the scene later. They also need to verify that the algorithm is applying the correct colors and make any corrections necessary to the output.\n",
    "</p>\n",
    "<p>\n",
    "BentoColor aims to address the issues with state of the art colorization technology by automatically detecting what color the objects in each frame should be. This can be used for each frame independantly, or be used in conjunction with the current technology to track the generated colors throughout the scene.\n",
    "</p>\n",
    "\n",
    "<h3>BentoColor Technology</h3>\n",
    "<p>\n",
    "In order to automatically colorize images and video frames, BentoColor will utilize a neural network. Specifically, Bentocolor will utilize an architecture similar to the U-net architecture described in <a href=https://arxiv.org/pdf/1505.04597.pdf>this paper</a> and <a href=https://www.tensorflow.org/tutorials/generative/pix2pix>this tutorial</a>. The U-net architecture is composed of a downsampling section which extracts features from the input image, an upsampling section which determines what color should be applied to each feature, and skip connections between them which allow the neural network to pass information about important patterns in the image forward.\n",
    "</p>\n",
    "\n",
    "<h3>General Issues</h3>\n",
    "\n",
    "<p>\n",
    "There are a few other issues associated with colorization which are not exclusive to machine learning. For example, what color is a shoe? This is a harder question to answer than it seems because there really is no typical shoe color. While the typical boot may be brown, one could color it black or light beige and no one would think twice about it. This may seem like it would make the task of colorization easier, but in fact it just makes the patterns more difficult to detect if the machine learning model sees the same shoe in many different colors. This applies to many common objects, not just clothing and is a substantial issue with machine learning based colorization.\n",
    "</p>\n",
    "<h1>Image Colorization</h1>\n",
    "<h3>Image Data</h3>\n",
    "\n",
    "BentoColor used two sources of data for this project. \n",
    "<p>\n",
    "<a href=https://www.kaggle.com/datasets/utkarshsaxenadn/landscape-recognition-image-dataset-12k-images>This dataset</a> from kaggle was used to develop a proof-of-concept model capable of fully colorizing images. The dataset contains 12,000 images of landscapes that fall into five categories:\n",
    "Coast, Desert, Forest, Glacier, and Mountain. These images were divided into a training set containing 10,000 images, a validation set containing 1,500 images, and a test set containing 500 images. While training and testing this model, due primarily to a lack of computing resources, the resolution of these images was reduced to 64x64 pixels. The images were also augmented before being shown to the model by randomly rotating and flipping them around both the horizontal and vertical axes. This was done to prevent the model from overfitting as well as to increase the effective number of images the model could train on.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_paths = random.sample(os.listdir('../images/train_images/'),12)\n",
    "example_paths = ['../images/train_images/' + filename for filename in example_paths]\n",
    "example_images = [utils.load_and_resize(path,transform=True) for path in example_paths]\n",
    "\n",
    "fig, ax = plt.subplots(3,4,figsize=(16,12))\n",
    "for i, row in enumerate(ax):\n",
    "    for j, col in enumerate(row):\n",
    "        col.imshow(example_images[j+3*i])\n",
    "        col.set_xticks([])\n",
    "        col.set_yticks([])\n",
    "fig.set_facecolor('#FFFFFF')\n",
    "fig.subplots_adjust(wspace=0.02, hspace=0.2)\n",
    "fig.suptitle('Example Training Images',fontsize=48);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Color Formats</h3>\n",
    "\n",
    "<p>\n",
    "In addition to being scaled down to 64x64 pixels and flipped or rotated, the image data was further transformed. The LAB color space was selected for this work rather than the RGB color space, so each image needed to be converted to this color space using the <a href=https://pypi.org/project/opencv-python/>opencv package</a>. The LAB color space was selected because it encodes the image as a grayscale part (L channel) and two color parts (A and B channels). This was helpful because the L channel could be extracted from the newly encoded image and used as the input for the model while training. In addition, the model would only need to predict two values (A and B) for the image color as opposed to the three it would need to predict for RGB. The original L channel could be re-combined with the model predicted A and B channels to produce the fully colorized image.\n",
    "</p>\n",
    "\n",
    "\n",
    "<h1>Using Neural Newtorks</h1>\n",
    "\n",
    "<h3>Neural Network Layers</h3>\n",
    "\n",
    "<p>\n",
    "The following cell shows the basic downsampling and upsampling building blocks used to build out the neural newtorks. The downsampling layers are simply convolutional layers. These layers are normalized and utilize a leaky relu activation function. The upsampling layers are only used in the generators and are more complicated because they include deconvolutional layers and the skip connections that make the U-Net architecture function. This skip connection comes from a downsampling layer earlier in the neural network and allows for the U-Net to pass information about important patterns in the image forward.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Downsampling block for use in generator and discriminator\n",
    "def downsampling(filters,stride,prev_layer):\n",
    "\n",
    "    init = initializers.RandomNormal()\n",
    "\n",
    "    block = layers.Conv2D(filters,strides=stride,kernel_size=4,padding='same',kernel_initializer=init,use_bias=False)(prev_layer)\n",
    "    block = layers.BatchNormalization()(block)\n",
    "    block = layers.LeakyReLU(0.2)(block)\n",
    "\n",
    "    return block\n",
    "\n",
    "# Upsampling block for use in generator only\n",
    "# Skip layer should be the same shape as the output to the transpose convolutional layer\n",
    "# Or twice the size of the prev_layer input\n",
    "def upsampling(filters,stride,prev_layer,skip_layer):\n",
    "\n",
    "    init = initializers.RandomNormal()\n",
    "\n",
    "    block = layers.Conv2DTranspose(\n",
    "        filters,strides=stride,kernel_size=4,padding='same',kernel_initializer=init,use_bias=False)(prev_layer)\n",
    "    block = layers.BatchNormalization()(block)\n",
    "    block = layers.Concatenate()([block,skip_layer])\n",
    "    block = layers.LeakyReLU(0.2)(block)\n",
    "    block = layers.Dropout(0.3)(block)\n",
    "\n",
    "    return block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generator Architecure</h3>\n",
    "\n",
    "<p>\n",
    "The following generator architecture based on the scientific literature is used in all attempts to colorize the images. It takes in one grayscale channel and outputs two color channels all of size 64x64 pixels. \n",
    "</p>\n",
    "\n",
    "<img src=resources/generator_arch.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "\n",
    "    init = initializers.RandomNormal()\n",
    "\n",
    "    # Model input is a (64x64) grayscale image\n",
    "    model_input = layers.Input(shape=(64,64,1))\n",
    "\n",
    "    # Downsampling stack\n",
    "    down0 = downsampling(filters=32,stride=1,prev_layer=model_input) # (64x64) -> (64x64)\n",
    "    down1 = downsampling(64,2,down0) # (64x64) -> (32x32)\n",
    "    down2 = downsampling(128,2,down1) # (32x32) -> (16x16)\n",
    "    down3 = downsampling(256,2,down2) # (16x16) -> (8x8)\n",
    "    down4 = downsampling(256,2,down3) # (8x8) -> (4x4)\n",
    "    down5 = downsampling(256,2,down4) # (4x4) -> (2x2)\n",
    "    down6 = downsampling(256,2,down5) # (2x2) -> (1x1)\n",
    "\n",
    "\n",
    "    # Upsampling stack\n",
    "    up5 = upsampling(filters=256,stride=2,prev_layer=down6,skip_layer=down5) # (1x1) -> (2x2)\n",
    "    up4 = upsampling(256,2,up5,down4) # (2x2) -> (4x4)\n",
    "    up3 = upsampling(256,2,up4,down3) # (4x4) -> (8x8)\n",
    "    up2 = upsampling(128,2,up3,down2) # (8x8) -> (16x16)\n",
    "    up1 = upsampling(64,2,up2,down1) # (16x16) -> (32x32)\n",
    "    up0 = upsampling(32,2,up1,down0) # (32x32) -> (64x64)\n",
    "    \n",
    "    # Model output is (64x64) with 2 color channels with values between -1 and 1\n",
    "    model_output = layers.Conv2DTranspose(\n",
    "        2,strides=1,kernel_size=4,padding='same',activation='tanh',kernel_initializer=init,use_bias=False)(up0)\n",
    "\n",
    "    model = keras.models.Model(model_input,model_output)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Mean Squared Error Training</h1>\n",
    "\n",
    "<p>\n",
    "Training began with the landscape colorizing model. The first attempt at training the model used a mean squared error loss function with the simple goal of making the output colorized image as close to the original color image as possible. Colorized images from the test set resulting from this model can be seen below alongsize the original color and original grayscale images that they were produced from. While this method worked reasonably well, the model tended to overfit to the training images, and images from the validation set appeared less vibrant and more washed-out than the original color images in general.\n",
    "</p>\n",
    "\n",
    "<img src=resources/mse_graphic.png width=300></img>\n",
    "\n",
    "<p>\n",
    "<b>More information on the model trained using mean squared error as well as the code used to train this model can be found in <a href=./MSE_Colorizer.ipynb>this notebook</a>.</b>\n",
    "</p>\n",
    "\n",
    "<h3>MSE Training Results</h3>\n",
    "\n",
    "<p>The chart below shows the model's (mean squared error) loss plotted against the number of epochs the model was trained for. The clear downward trend signals that the model's output images are getting closer to the original images. The training of this model was ended because the improvements in loss were beginning to slow down and training the model for much longer could easily lead to overfitting.\n",
    "</p>\n",
    "\n",
    "<img src=resources/MSE_Loss_TBoard.jpg width=600></img>\n",
    "\n",
    "<h1>Generative Adversarial Network Training</h1>\n",
    "\n",
    "<p>\n",
    "A modified GAN (Generative Adversarial Network) architecture based on <a href=https://arxiv.org/pdf/1611.07004.pdf>this paper</a> was the next step in training the colorization model. Rather than trying to make the input and output images as close as possible on a pixel-value level, the GAN architecture is actually composed of two neural networks. One network is the original U-net generator, and the other network is a standard convolutional neurual network or discriminator.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The generator has the goal of colorizing the images in this architecture, and can be thought of as a forger making fake colored images.\n",
    "</p>\n",
    "\n",
    "<p>The discriminator has the goal of determining whether a color image provided to it was produced by the generator. It can be thought of as a policeman.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The generator and disciminator are set against eachother in this case. The generator trains to produce images that the discriminator thinks are original or genuine. The discriminator trains to tell the images the generator produces apart from the genuine ones. As the generator gets better at producing realistic looking images, the discriminator needs to get better at telling them apart from real ones. This leads to a loop of both models constantly improving until the generator colorized images are actually indistinguishable from the real images.\n",
    "</p>\n",
    "\n",
    "<img src=resources/Gan_Arch.png width=250></img>\n",
    "\n",
    "<h3>Discriminator Architecure</h3>\n",
    "\n",
    "<p>\n",
    "The following discriminator architecture based on the scientific literature is used in all adversarial attempts to colorize images and videos. It takes in three 64x64 pixel channels and outputs an 8x8 array of unbounded logits that describe whether the discriminator thinks a certain section of the image it recieved was original or created by the generator.\n",
    "</p>\n",
    "\n",
    "<img src=resources/discriminator_arch.png></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "\n",
    "    init = initializers.RandomNormal()\n",
    "\n",
    "    gray_input = layers.Input(shape=(64,64,1))\n",
    "    color_input = layers.Input(shape=(64,64,2))\n",
    "\n",
    "    # Gray and colored inputs are combined\n",
    "    concat_input = layers.concatenate([gray_input,color_input])\n",
    "\n",
    "    # Downsampling stack\n",
    "    down0 = downsampling(filters=64,stride=2,prev_layer=concat_input) # (64x64) -> (32x32)\n",
    "    down1 = downsampling(128,2,down0)        # (32x32) -> (16x16)\n",
    "    down2 = downsampling(256,2,down1)        # (16x16) -> (8x8)\n",
    "\n",
    "    # Model output is (8x8) with unbounded values (no activation)\n",
    "    model_output = layers.Conv2D(1,2,strides=1,padding='same',kernel_initializer=init,use_bias=False)(down2)\n",
    "\n",
    "    model = keras.Model(inputs=[gray_input,color_input],outputs=model_output)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Advantages of GANs</h3>\n",
    "\n",
    "<p>\n",
    "The main advantage of using a GAN is that they are able to generate data that is very similar to existing data. If the generator is able to fool a well trained discriminator, it is highly likely that it will be able to fool an untrained person. GANs are also capable of producing decent results when the training dataset is smaller than desired. They are excellent at picking up on and reproducing similar patterns to those present in the training data.\n",
    "</p>\n",
    "\n",
    "<h3>Disadvantages of GANs</h3>\n",
    "\n",
    "<p>\n",
    "GANs are very difficult to train and have a variety of potential failure modes. One example is if either the generator or discriminator are too strong relative to the other, the weaker neural network will not be able to learn well. This is more of an issue for the generator because the discriminator's job is typically easier at the beginning of training when the generator is producing poorly colorized images. For this reason, the loss function for the generator also included a scaled factor of mean squared error to push it in the right direction at the start.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Because GANs are composed of two neural networks, they take even longer to train than the typical model. While the optimal number of epochs or time for training depends on your model architecture and dataset, it is not uncommon for neural networks, let alone GANs to take many hours to converge. Therefore, it is vital to select sizes for each neural network and for the dataset that will allow for convergence within a reasonable amount of time on the available hardware. This is further complicated because it is difficult to know how long it will take to train a neural network before attempting to train it. It is advisable to not use too small of a dataset or resolution, however as the neural network will not learn well if there is not enough information for it to learn from or not enough parameters in the network to learn most of the patterns present within the data.\n",
    "</p>\n",
    "\n",
    "<h3>Adversarial Generator Training</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_output_generated,generator_output,color_channels):\n",
    "\n",
    "    # Part of the loss is based on the discriminator output on fake images\n",
    "    cross_entropy = losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.1)\n",
    "\n",
    "    # Part of the loss (scaled) is based on the difference between the\n",
    "    # generated images and the original colored images\n",
    "    mse = losses.MeanSquaredError()\n",
    "    mse_scaler = 25\n",
    "\n",
    "    # Generator wants the discriminator to classify the generated images as 1 (real)\n",
    "    # Adersarial loss is the defference between all 1s and the actual discriminator output\n",
    "    adversarial_loss = cross_entropy(tf.ones_like(disc_output_generated),disc_output_generated)\n",
    "    mse_loss = mse(generator_output,color_channels) \n",
    "\n",
    "    # Return all three losses for tensorboard\n",
    "    return adversarial_loss + (mse_scaler * mse_loss), adversarial_loss, mse_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>\n",
    "The generator recieves the gray channel and outputs two color channels. This output is recombined with the original gray channels and sent to the discriminator with unmodified images. The loss for the generator is calculated based on how many generator colorized images the discriminator thinks are original and how many original images the discriminator thinks the generator colored. In addition, the mean squared error between the generated images and the real images is calculated for each generated image. This value is weighted and added to the generator loss calculated using the discriminator. The chart below demonstrates the training loop for the generator in this adversarial network.\n",
    "</p>\n",
    "\n",
    "<img src=resources/Gen_Training_Loop.png width=400></img>\n",
    "\n",
    "<h3>Discriminator Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_output_generated,disc_output_real):\n",
    "\n",
    "    cross_entropy = losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.1)\n",
    "\n",
    "    # Discriminator wants to classify real images as 1 and generated\n",
    "    # images as 0. Loss is the difference between the desired outputs\n",
    "    # and the actual outputs\n",
    "    real_loss = cross_entropy(tf.ones_like(disc_output_real),disc_output_real)\n",
    "    generated_loss = cross_entropy(tf.zeros_like(disc_output_generated),disc_output_generated)\n",
    "\n",
    "    total_loss = real_loss + generated_loss\n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The discriminator recieves both real images and images colorized by the generator. The goal of the discriminator is to classify patches of images it thinks are original as 1 and classify patches of images it thiks the generator colorized as 0. Patches were used instead of the entire image being classified as real or fake to help the generator determine which parts of the image specifically are poorly colorized and need more focus. The discriminator is then updated so that it classifies more original images as 1 and generated images as 0 using cross entropy loss.</p>\n",
    "\n",
    "<h3>Adversarial Training Results</h3>\n",
    "<img src=resources/Disc_Training_Loop.png width=400></img>\n",
    "<p>\n",
    "The four charts below show the discriminator loss, generator loss(adversarial), generator loss (based on MSE), and total generator loss respectively.\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "<li>The discriminator loss starts high and gets lower over time as the discriminator learns to tell the generator's fabrications apart from the original images.</li>\n",
    "<li>The generator's adversarial loss is the inverse of the discriminator's loss. It goes up over time because the generator's job is much harder than the discriminator's so it learns slower.</li>\n",
    "<li>The generator's MSE loss goes down over time as the generated images get closer to the original images. It eventually flattens out and increases slightly as the generator learns from the discriminator rather than the mean squared error. This is what allows the images to become so vibrant and detailed.</li>\n",
    "<li>The generator's total loss is the weighted sum of the adversarial and MSE components. The MSE component is larger initially, but the adversarial component eventually takes over.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<img src=resources/GAN_Loss_TBoard.jpg></img>\n",
    "\n",
    "<p>\n",
    "<b>More information on the model trained using the GAN architecture (with a mean squared term in the loss function) as well as the code used to train this model can be found in <a href=./GAN_Colorizer.ipynb>this notebook</a>.</b>\n",
    "</p>\n",
    "\n",
    "<h1>Image Colorization Results</h1>\n",
    "\n",
    "<p>\n",
    "Results for both the mean squared error only model, and the GAN model can be seen below. The leftmost image is the grayscale image that was input into both models. The middle two images are the output of each respective model. The rightmost image is the original image in full color. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_gen = load_model('models/1-5-23/mse/generator.h5')\n",
    "gan_gen = load_model('models/1-5-23/gan/generator.h5')\n",
    "\n",
    "for test_batch in range(2):\n",
    "    test_images = utils.image_loader(directory='../images/test_images/',\n",
    "     batch_size=8,training=False).__getitem__(test_batch)\n",
    "    utils.display_images(gray_channel=test_images[0],color_channels=test_images[1],\n",
    "    generator1=mse_gen,gen1_title='MSE Colorized',generator2=gan_gen,gen2_title='GAN Colorized');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Interpreting the Results</h3>\n",
    "\n",
    "<p>\n",
    "<b>The interpretation of these results and the relative success of each model is highly subjective. Each person viewing the results may prefer a different colorization for any number of reasons.</b>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "There are several important questions to ask when determining how well a given model can colorize images. Here are a few important ones one should consider when assessing these models:\n",
    "</p>\n",
    "<ul>\n",
    "<li>How close are the colors in the model's test output to the test image's original colors?</li>\n",
    "<li>Are the differences in the model's test output to the test image's original colors reasonable or expected?</li>\n",
    "<li>Are there any major defects in the model's output (random lines, splotches, ect...)?</li>\n",
    "<li>Are the produced colors vibrant or dull when either is appropriate?</li>\n",
    "<li>Is there an appropriate variety of color to capture the detail in each image</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "In this case, there are not many major defects in most of the colorized images. The GAN seems to be producing more vibrant and detailed images while the model trained using mean squared error seems to be using fewer colors overall. The GAN also seems to be segmenting most of the images into its constituent subjects better and coloring each of them differently while the model trained using mean squared error tends to select one predominant color and apply that to more of the image. The GAN does seem more likely to deviate from the colors in the original test image, but it often uses colors that could also be appropriate for the objects in scene.\n",
    "</p>\n",
    "\n",
    "<p><b>\n",
    "For these reasons, the GAN will be the primary model utilized for colorizing videos.</b>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Video Colorization</h1>\n",
    "\n",
    "<p>\n",
    "Following the success of using a GAN to colorize still images, BentoColor has decided to utilize the same technology for video colorization. This is possible because a video is in essence just a series of frames which are still images. If each of these images can be successfully colorized, they can theoretically be arranged to form a fully colorized video. </p>\n",
    "\n",
    "<p>\n",
    "Unsurprisingly, colorizing videos poses its own unique challenges. If a video is colorized frame-by-frame there is no guarantee that there will be any consistency between the frames in each scene. Regredless of how nice each frame looks independently, this can lead to a choppy or flashing video which is not acceptable in a colorization product for videos. This can be highly detrimental if additional strategies aimed at mitigating this are not employed.\n",
    "</p>\n",
    "\n",
    "<h3> Video Data</h3>\n",
    "\n",
    "<img src=resources/dusty_logo.jpg width=200></img>\n",
    "<p>\n",
    "The television show <a href=https://en.wikipedia.org/wiki/Dusty%27s_Trail>Dusty's Trail</a> is the second source of data used for this project. This was chosen primarily because it is one of very few colored television shows or movies which are public domain. Training data was extracted from episodes 3 through 6 using FFmpeg. Eight frames were extracted per second of video resulting in approximately 11,000 frames per 22-ish minute episode.\n",
    "</p>\n",
    "\n",
    "<code>ffmpeg -i 'Input Video Path' -vf fps=8 'Output Image Path .jpeg'</code>\n",
    "<p>\n",
    "These frames were then converted to 64x64 pixels due to a lack of computing power available to train a larger model. The same augmentations were performed as to the landscape images; each image could be randomly rotated or flipped around either axis. The rotation was potentially unnecessary as the orientation of objects in this television show is typically right-side-up as opposed to the landscape dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_vid_paths = random.sample(os.listdir('../shows/episode3/'),12)\n",
    "example_vid_paths = ['../shows/episode3/' + filename for filename in example_vid_paths]\n",
    "example_frames = [utils.load_and_resize(path,transform=True) for path in example_vid_paths]\n",
    "\n",
    "fig, ax = plt.subplots(3,4,figsize=(16,12))\n",
    "for i, row in enumerate(ax):\n",
    "    for j, col in enumerate(row):\n",
    "        col.imshow(example_frames[j+3*i])\n",
    "        col.set_xticks([])\n",
    "        col.set_yticks([])\n",
    "fig.set_facecolor('#FFFFFF')\n",
    "fig.subplots_adjust(wspace=0.02, hspace=0.2)\n",
    "fig.suptitle('Example Video Frames',fontsize=48);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generative Adversarial Network Training</h1>\n",
    "\n",
    "<p>BentoColor will utilize a U-Net model trained with the same GAN architecture described in the image colorization section. Once again, the GAN was selected because it seems to be producing more vibrant and detailed images than the model trained using only mean squared error. A lower weight for the mean squared error was used in the loss function for this GAN. This was done because the training images were less varied than in the previous GAN's case. BentoColor feared that relying too heavily on mean squared error for loss in this case would lead to overfitting so adversarial loss was prioritized.\n",
    " </p>\n",
    "\n",
    "<img src=resources/Gan_Arch.png width=250></img>\n",
    "\n",
    "<p>\n",
    "Similar to the previous gan, he four charts below show the discriminator loss, generator loss(adversarial), generator loss (based on MSE), and total generator loss respectively.\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "<li>The discriminator loss starts high and gets lower over time as the discriminator learns to tell the generator's fabrications apart from the original images.</li>\n",
    "<li>The generator's adversarial loss is overall lower than the original GAN's. This is likely because there is less diversity in the training images making the generator learn more quickly.</li>\n",
    "<li>The generator's MSE loss is also lower overall than the original GAN's. Once again, this is likely because there is less diversity in the training images making the generator likely to overfit.</li>\n",
    "<li>The generator's total loss is the weighted sum of the adversarial and MSE components (different weights than the previous GAN). The MSE component is larger initially, but the adversarial component eventually takes over.</li>\n",
    "</ul>\n",
    "\n",
    "<img src=resources/Dusty_Loss_TBoard.jpg></img>\n",
    "\n",
    "<p>\n",
    "<b>More information on the model trained using the GAN architecture (with a mean squared term in the loss function) as well as the code used to train this model can be found in <a href=./Dusty_Colorizer.ipynb>this notebook</a>.</b>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Grayscale Video</th>\n",
    "        <th>Colorized Video</th>\n",
    "        <th>Original Video</th>\n",
    "    <tr>\n",
    "        <td><img src=resources/video/test_2_grayscale.gif width=256></img></td>\n",
    "        <td><img src=resources/video/test_2_colorized.gif width=256></img></td>\n",
    "        <td><img src=resources/video/test_2_original.gif width=256></img></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Grayscale Video</th>\n",
    "        <th>Colorized Video</th>\n",
    "        <th>Original Video</th>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td><img src=resources/video/test_7_grayscale.gif width=256></img></td>\n",
    "        <td><img src=resources/video/test_7_colorized.gif width=256></img></td>\n",
    "        <td><img src=resources/video/test_7_original.gif width=256></img></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Grayscale Video</th>\n",
    "        <th>Colorized Video</th>\n",
    "        <th>Original Video</th>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td><img src=resources/video/test_18_grayscale.gif width=256></img></td>\n",
    "        <td><img src=resources/video/test_18_colorized.gif width=256></img></td>\n",
    "        <td><img src=resources/video/test_18_original.gif width=256></img></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Grayscale Video</th>\n",
    "        <th>Colorized Video</th>\n",
    "        <th>Original Video</th>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td><img src=resources/video/test_28_grayscale.gif width=256></img></td>\n",
    "        <td><img src=resources/video/test_28_colorized.gif width=256></img></td>\n",
    "        <td><img src=resources/video/test_28_original.gif width=256></img></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Interpreting the Results</h3>\n",
    "\n",
    "<p>\n",
    "<b>The interpretation of these results and the relative success of each model is highly subjective. Each person viewing the results may prefer a different colorization for any number of reasons.</b>\n",
    "</p>\n",
    "\n",
    "<p>The video colorizing model can be assessed by asking many of the same questions as before:</p>\n",
    "\n",
    "<ul>\n",
    "<li>How close are the colors in the model's test output to the test image's original colors?</li>\n",
    "<li>Are the differences in the model's test output to the test image's original colors reasonable or expected?</li>\n",
    "<li>Are there any major defects in the model's output (random lines, splotches, ect...)?</li>\n",
    "<li>Are the produced colors vibrant or dull when either is appropriate?</li>\n",
    "<li>Is there an appropriate variety of color to capture the detail in each image</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "With videos, however consistency and stability of the colors between frames is also extremely important. While many frames look good and most of the colors are in the correct places, the inconsistency between frames is definitely substantial and a major weakness of this model.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "While this model was trained on over 40,000 frames, many of these frames are likely from the same scenes with only small variations in color. While BentoColor did their best augmenting the image data with random flips and rotations, there is only so much that can be expected from a model trained using limited data and computing power. It also does not look like the GAN has converged completely and could run for thousands of more epochs before it would. This is apparent in the tensorboard logs above where both the generator adversarial loss and discriminator loss have not reached a steady state. It is impossible to know whether this would lead to better results or more overfitting though.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Recommendations</h1>\n",
    "\n",
    "The most important recommendation is for BentoColor to make partnerships with image and movie rightsholders to acquire more video data. Subsizide early colorizations in exchange for rights to train future models on shows and movies owned by these partners. This will help BentoColor acquire new customers and form relationships with them as well and provide the necessary data to train the next generation models.\n",
    "\n",
    "As far as the model itself, BentoColor should look into imcorporating a classifier into the U-net as proposed by <a href=https://arxiv.org/abs/1712.03400>this paper</a>. Currently, the model does not know what it is actually coloring and is only selecting colors based on patterns of pixels in the image. Incorporating a classifier could give the model context into what it is actually \"looking at\" in different parts of the image improving its performance especially when multiple objects are in frame. This was impossible to incorporate for this project because most pre-trained classifiers that the paper suggests using require an image resolution higher than my workstation can handle. Additional technical challenges with incorporating this into the model include color space conversions and selecting the appropriate pre-trained classifier for the subject the model will be colorizing.\n",
    "\n",
    "Finally, while the colorization model is definitely not ready for production, it can be used to seed current techniques. While the model does not do well with videos, it colorizes pictures reasonably well. The model can be used to create seed frames that can help artists using current colorization techniques with inspiration and give them a starting point. It is unclear whether fully automatic AI image and video colorization will be possible in the near future, but the incomplete version can still be a very useful tool in the hands of talented technicians and artists today."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://www.flaticon.com/free-icon/bento_4073597?term=bento&page=1&position=2&origin=tag&related_id=4073597\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/1712.03400\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1611.07004.pdf\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/tutorials/generative/pix2pix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
