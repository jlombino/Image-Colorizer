{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Image colorization using a Generative Adversarial Network (GAN)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import colorizerutils as utils\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'>This VRAM limit is necessary because of my atypical hardware setup. You may need to remove or edit this to your desired VRAM usage limit.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of GPU memory to allocate to Tensorflow\n",
    "vram = 14 # GB\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.set_logical_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.LogicalDeviceConfiguration(memory_limit=vram*1024)])\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataset using images from ../images/train_images/\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    utils.image_loader,\n",
    "    output_types=(tf.float32,tf.float32)\n",
    ")\n",
    "\n",
    "# Directory for tensorboard logs\n",
    "log_directory = 'logs/'\n",
    "\n",
    "# Set up tensorboard logging for the generator and discriminator loss\n",
    "summary_writer = tf.summary.create_file_writer(log_directory + 'gan_colorizer/')\n",
    "gen_loss_tracker = metrics.Mean('Generator_loss',dtype=tf.float32)\n",
    "gen_adv_tracker = metrics.Mean('Generator_adversarial_loss',dtype=tf.float32)\n",
    "gen_mse_tracker = metrics.Mean('Generator_mse_loss',dtype=tf.float32)\n",
    "disc_loss_tracker = metrics.Mean('Discriminator_loss',dtype=tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network Structure</h1>\n",
    "The colorizer uses a generative adversarial network (GAN) which contains a generator and a discriminator. The general structure of a GAN is shown here.<br><br>\n",
    "<center><img src=resources/Gan_Arch.png width=200><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling block for use in generator and discriminator\n",
    "def downsampling(filters,stride,prev_layer):\n",
    "\n",
    "    init = initializers.RandomNormal()\n",
    "\n",
    "    block = layers.Conv2D(filters,strides=stride,kernel_size=4,padding='same',kernel_initializer=init,use_bias=False)(prev_layer)\n",
    "    block = layers.BatchNormalization()(block)\n",
    "    block = layers.LeakyReLU(0.2)(block)\n",
    "\n",
    "    return block\n",
    "\n",
    "# Upsampling block for use in generator only\n",
    "# Skip layer should be the same shape as the output to the transpose convolutional layer\n",
    "# Or twice the size of the prev_layer input\n",
    "def upsampling(filters,stride,prev_layer,skip_layer):\n",
    "\n",
    "    init = initializers.RandomNormal()\n",
    "\n",
    "    block = layers.Conv2DTranspose(\n",
    "        filters,strides=stride,kernel_size=4,padding='same',kernel_initializer=init,use_bias=False)(prev_layer)\n",
    "    block = layers.BatchNormalization()(block)\n",
    "    block = layers.Concatenate()([block,skip_layer])\n",
    "    block = layers.LeakyReLU(0.2)(block)\n",
    "    block = layers.Dropout(0.3)(block)\n",
    "\n",
    "    return block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generator</h3>\n",
    "The generator uses the modified U-Net architecture pictured here.<br><br>\n",
    "<center><img src=resources/generator_arch.png><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "\n",
    "    init = initializers.RandomNormal()\n",
    "\n",
    "    # Model input is a (64x64) grayscale image\n",
    "    model_input = layers.Input(shape=(64,64,1))\n",
    "\n",
    "    # Downsampling stack\n",
    "    down0 = downsampling(filters=32,stride=1,prev_layer=model_input) # (64x64) -> (64x64)\n",
    "    down1 = downsampling(64,2,down0) # (64x64) -> (32x32)\n",
    "    down2 = downsampling(128,2,down1) # (32x32) -> (16x16)\n",
    "    down3 = downsampling(256,2,down2) # (16x16) -> (8x8)\n",
    "    down4 = downsampling(256,2,down3) # (8x8) -> (4x4)\n",
    "    down5 = downsampling(256,2,down4) # (4x4) -> (2x2)\n",
    "    down6 = downsampling(256,2,down5) # (2x2) -> (1x1)\n",
    "\n",
    "\n",
    "    # Upsampling stack\n",
    "    up5 = upsampling(filters=256,stride=2,prev_layer=down6,skip_layer=down5) # (1x1) -> (2x2)\n",
    "    up4 = upsampling(256,2,up5,down4) # (2x2) -> (4x4)\n",
    "    up3 = upsampling(256,2,up4,down3) # (4x4) -> (8x8)\n",
    "    up2 = upsampling(128,2,up3,down2) # (8x8) -> (16x16)\n",
    "    up1 = upsampling(64,2,up2,down1) # (16x16) -> (32x32)\n",
    "    up0 = upsampling(32,2,up1,down0) # (32x32) -> (64x64)\n",
    "    \n",
    "    # Model output is (64x64) with 2 color channels with values between -1 and 1\n",
    "    model_output = layers.Conv2DTranspose(\n",
    "        2,strides=1,kernel_size=4,padding='same',activation='tanh',kernel_initializer=init,use_bias=False)(up0)\n",
    "\n",
    "    model = keras.models.Model(model_input,model_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_output_generated,generator_output,color_channels):\n",
    "\n",
    "    # Part of the loss is based on the discriminator output on fake images\n",
    "    cross_entropy = losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.1)\n",
    "\n",
    "    # Part of the loss (scaled) is based on the difference between the\n",
    "    # generated images and the original colored images\n",
    "    mse = losses.MeanSquaredError()\n",
    "    mse_scaler = 100\n",
    "\n",
    "    # Generator wants the discriminator to classify the generated images as 1 (real)\n",
    "    # Adersarial loss is the defference between all 1s and the actual discriminator output\n",
    "    adversarial_loss = cross_entropy(tf.ones_like(disc_output_generated),disc_output_generated)\n",
    "    mse_loss = mse(generator_output,color_channels) \n",
    "\n",
    "    # Return all three losses for tensorboard\n",
    "    return adversarial_loss + (mse_scaler * mse_loss), adversarial_loss, mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator and generator optimizer\n",
    "generator = make_generator()\n",
    "\n",
    "# Beta1 to decrease importance of previous batches\n",
    "generator_optimizer = optimizers.Adam(2e-4,beta_1=0.5)\n",
    "generator.compile()\n",
    "generator.summary()\n",
    "#keras.utils.plot_model(generator,show_shapes=True,to_file='generator.png',dpi=48)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Discriminator</h3>\n",
    "The discriminator uses the typical convolutional neural network architecture shown here.<br><br>\n",
    "<center><img src=resources/discriminator_arch.png><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "\n",
    "    init = initializers.RandomNormal()\n",
    "\n",
    "    gray_input = layers.Input(shape=(64,64,1))\n",
    "    color_input = layers.Input(shape=(64,64,2))\n",
    "\n",
    "    # Gray and colored inputs are combined\n",
    "    concat_input = layers.concatenate([gray_input,color_input])\n",
    "\n",
    "    # Downsampling stack\n",
    "    down0 = downsampling(filters=64,strides=2,prev_layer=concat_input) # (64x64) -> (32x32)\n",
    "    down1 = downsampling(128,2,down0)        # (32x32) -> (16x16)\n",
    "    down2 = downsampling(256,2,down1)        # (16x16) -> (8x8)\n",
    "    down3 = downsampling(256,2,down1)        # (8x8) -> (4x4)\n",
    "\n",
    "    # Model output is (4x4) with unbounded values (no activation)\n",
    "    model_output = layers.Conv2D(1,2,strides=1,padding='same',kernel_initializer=init,use_bias=False)(down3)\n",
    "\n",
    "    model = keras.Model(inputs=[gray_input,color_input],outputs=model_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_output_generated,disc_output_real):\n",
    "\n",
    "    cross_entropy = losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.1)\n",
    "\n",
    "    # Discriminator wants to classify real images as 1 and generated\n",
    "    # images as 0. Loss is the difference between the desired outputs\n",
    "    # and the actual outputs\n",
    "    real_loss = cross_entropy(tf.ones_like(disc_output_real),disc_output_real)\n",
    "    generated_loss = cross_entropy(tf.zeros_like(disc_output_generated),disc_output_generated)\n",
    "\n",
    "    total_loss = real_loss + generated_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the discriminator and discriminator optimizer\n",
    "discriminator = make_discriminator()\n",
    "discriminator_optimizer = optimizers.Adam(2e-4)\n",
    "discriminator.compile()\n",
    "discriminator.summary()\n",
    "#keras.utils.plot_model(discriminator,show_shapes=True,to_file='discriminator.png',dpi=48)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Training Loop</h1><br>\n",
    "This will train the models using all available training images and save the training results as a tensorboard log. The fully trained generator and discriminator are also saved, but the generator is the more useful model because it produces the colorized images.\n",
    "<h3>Generator Training Loop</h3>\n",
    "<center><img src=resources/Gen_Training_Loop.png width=300></center>\n",
    "<h3>Discriminator Training Loop</h3>\n",
    "<center><img src=resources/Disc_Training_Loop.png width=300></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(gray_channel,color_channels):\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        # Generate a batch of fake images\n",
    "        generator_output = generator(gray_channel,training=True)\n",
    "\n",
    "        # Get discriminator output on real and generated images\n",
    "        disc_output_real = discriminator([gray_channel,color_channels],training=True)\n",
    "        disc_output_generated = discriminator([gray_channel,generator_output],training=True)\n",
    "\n",
    "        # Calculate loss for generator and discriminator based on discriminator outputs\n",
    "        gen_loss, adversarial_loss, mse_loss = generator_loss(disc_output_generated,generator_output,color_channels)\n",
    "        disc_loss = discriminator_loss(disc_output_generated,disc_output_real)\n",
    "\n",
    "    # Calculate generator gradients and train generator\n",
    "    generator_gradients = gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,generator.trainable_variables))\n",
    "\n",
    "    # Calculate disciminator gradients and train discriminator\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,discriminator.trainable_variables))\n",
    "\n",
    "    # Update tensorboard losses\n",
    "    gen_loss_tracker(gen_loss)\n",
    "    gen_adv_tracker(adversarial_loss)\n",
    "    gen_mse_tracker(mse_loss)\n",
    "    disc_loss_tracker(disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs,dataset):\n",
    "    \n",
    "    step_counter = 0\n",
    "\n",
    "    # Progress bar/tracking\n",
    "    for _ in tqdm(range(epochs)):\n",
    "\n",
    "        for gray_channel,color_channels in dataset:\n",
    "            train_step(gray_channel,color_channels)\n",
    "\n",
    "            # Tensorboard logging for training\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('Generator_loss',gen_loss_tracker.result(),step=step_counter)\n",
    "                tf.summary.scalar('Generator_adversarial_loss',gen_adv_tracker.result(),step=step_counter)\n",
    "                tf.summary.scalar('Generator_mse_loss',gen_mse_tracker.result(),step=step_counter)\n",
    "                tf.summary.scalar('Discriminator_loss',disc_loss_tracker.result(),step=step_counter)\n",
    "            \n",
    "            gen_loss_tracker.reset_states()\n",
    "            gen_adv_tracker.reset_states()\n",
    "            gen_mse_tracker.reset_states()\n",
    "            disc_loss_tracker.reset_states()\n",
    "            step_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(epochs=500,dataset=train_dataset)\n",
    "generator.save('models/gan/generator.h5')\n",
    "discriminator.save('models/gan/discriminator.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results for batches of the training set\n",
    "for train_batch in range(2):\n",
    "    train_images = utils.image_loader(directory='../images/train_images/',\n",
    "        batch_size=8,training=False).__getitem__(train_batch)\n",
    "    utils.display_images(gray_channel=train_images[0],color_channels=train_images[1],generator1=generator,gen1_title='GAN Colored');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validation Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results for batches of the validation set\n",
    "for val_batch in range(2):\n",
    "    val_images = utils.image_loader(directory='../images/val_images/',\n",
    "        batch_size=8,training=False).__getitem__(val_batch)\n",
    "    utils.display_images(gray_channel=val_images[0],color_channels=val_images[1],generator1=generator,gen1_title='GAN Colored');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
